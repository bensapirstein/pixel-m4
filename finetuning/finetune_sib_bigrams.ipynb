{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5b0172",
   "metadata": {},
   "source": [
    "# PIXEL-M4 Finetuning for SIB-200 Classification\n",
    "\n",
    "This notebook demonstrates how to finetune a PIXEL-M4 model for SIB-200 language classification using bigrams rendering. It's based on the `run_sib_bigrams.py` script but configured for notebook use with simple configuration instead of command-line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fd2b0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries including transformers, torch, datasets, and PIXEL-specific components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0480629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bens/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "from datasets import load_dataset, get_dataset_config_names, load_from_disk\n",
    "from evaluate import load as load_metric\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score as compute_f1_score\n",
    "\n",
    "# PIXEL imports\n",
    "from pixel import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Modality,\n",
    "    PangoCairoTextRenderer,\n",
    "    PIXELForSequenceClassification,\n",
    "    PIXELTrainer,\n",
    "    PIXELTrainingArguments,\n",
    "    PoolingMode,\n",
    "    PyGameTextRenderer,\n",
    "    get_attention_mask,\n",
    "    get_transforms,\n",
    "    glue_strip_spaces,\n",
    "    resize_model_embeddings,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerFast,\n",
    "    set_seed,\n",
    ")\n",
    "from pixel.data.rendering.pangocairo_renderer_bigrams_iso_char import PangoCairoTextRenderer as PangoCairoBigramsRenderer\n",
    "\n",
    "# Enable auto-reload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760280f5",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Configure all training parameters directly in the notebook instead of using command-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0207a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set for language: arz_Arab\n",
      "Model path: ../Team-PIXEL/pixel-m4\n",
      "Output directory: ../logs/pixel-m4/sib-200/arz_Arab\n",
      "Rendering backend: bigrams\n",
      "Max sequence length: 196\n",
      "Device: GPU\n"
     ]
    }
   ],
   "source": [
    "# Configuration constants\n",
    "SIB_200_HF_ID = \"Davlan/sib200\"\n",
    "\n",
    "# Configuration parameters\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model arguments\n",
    "    model_name_or_path: str = \"../Team-PIXEL/pixel-m4\"\n",
    "    config_name: Optional[str] = None\n",
    "    processor_name: Optional[str] = None\n",
    "    rendering_backend: str = \"bigrams\"  # Use bigrams rendering\n",
    "    fallback_fonts_dir: str = \"../fallback_fonts\"\n",
    "    render_rgb: bool = False\n",
    "    cache_dir: Optional[str] = None\n",
    "    model_revision: str = \"main\"\n",
    "    use_auth_token: Optional[str] = None\n",
    "    pooling_mode: str = \"mean\"\n",
    "    pooler_add_layer_norm: bool = True\n",
    "    dropout_prob: float = 0.1\n",
    "    \n",
    "    # Data arguments\n",
    "    data_dir: Optional[str] = None\n",
    "    language: str = \"arz_Arab\"  # Arabic (Egypt) as example\n",
    "    dataset_name: str = SIB_200_HF_ID\n",
    "    dataset_config_name: Optional[str] = None\n",
    "    max_seq_length: int = 196\n",
    "    overwrite_cache: bool = False\n",
    "    pad_to_max_length: bool = True\n",
    "    max_train_samples: Optional[int] = None\n",
    "    max_eval_samples: Optional[int] = None\n",
    "    max_predict_samples: Optional[int] = None\n",
    "    train_file: Optional[str] = None\n",
    "    validation_file: Optional[str] = None\n",
    "    test_file: Optional[str] = None\n",
    "    \n",
    "    # Training arguments\n",
    "    output_dir: str = f\"../logs/pixel-m4/sib-200/arz_Arab\"\n",
    "    overwrite_output_dir: bool = True\n",
    "    do_train: bool = True\n",
    "    do_eval: bool = True\n",
    "    do_predict: bool = True\n",
    "    per_device_train_batch_size: int = 16\n",
    "    per_device_eval_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 5e-5\n",
    "    num_train_epochs: int = 10\n",
    "    max_steps: int = -1\n",
    "    warmup_steps: int = 100\n",
    "    logging_steps: int = 100\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    save_strategy: str = \"steps\"\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_f1\"\n",
    "    greater_is_better: bool = True\n",
    "    early_stopping: bool = True\n",
    "    early_stopping_patience: int = 3\n",
    "    seed: int = 42\n",
    "    fp16: bool = True if torch.cuda.is_available() else False\n",
    "    dataloader_num_workers: int = 4\n",
    "    remove_unused_columns: bool = False\n",
    "    log_predictions: bool = False\n",
    "    report_to: str = \"none\"  # Can be \"wandb\" or \"tensorboard\" if desired\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "print(f\"Configuration set for language: {config.language}\")\n",
    "print(f\"Model path: {config.model_name_or_path}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Rendering backend: {config.rendering_backend}\")\n",
    "print(f\"Max sequence length: {config.max_seq_length}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e77124",
   "metadata": {},
   "source": [
    "## 3. Setup Logging and Seed\n",
    "\n",
    "Initialize logging and set random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44180ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 04:58:24 - INFO - __main__ - Training parameters configured\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Seed set to: 42\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Seed set to: 42\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Device: NVIDIA GeForce RTX 3090\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Using FP16: True\n",
      "Logging and seed setup complete!\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Device: NVIDIA GeForce RTX 3090\n",
      "11/15/2025 04:58:24 - INFO - __main__ - Using FP16: True\n",
      "Logging and seed setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "log_level = logging.INFO\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=log_level,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(log_level)\n",
    "\n",
    "# Set logging levels for libraries\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "logger.info(f\"Training parameters configured\")\n",
    "logger.info(f\"Seed set to: {config.seed}\")\n",
    "logger.info(f\"Device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
    "logger.info(f\"Using FP16: {config.fp16}\")\n",
    "\n",
    "print(\"Logging and seed setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b0df6",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Load the SIB-200 dataset for the specified language and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1342a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:07 - INFO - __main__ - Loading SIB-200 dataset for language: arz_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/bens/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/datasets/packaged_modules/csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.info - Loading Dataset Infos from /home/bens/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/datasets/packaged_modules/csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.info - Loading Dataset info from /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sib200 (/home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.builder - Found cached dataset sib200 (/home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.info - Loading Dataset info from /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-2777f9a8998d3d39.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-2777f9a8998d3d39.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-3ee3cba1a0d6deb6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-3ee3cba1a0d6deb6.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-805e85a19d1b07f4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-805e85a19d1b07f4.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/701 [00:00<?, ? examples/s]Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-96feb659c6969735.arrow\n",
      "Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-96feb659c6969735.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-96feb659c6969735.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 701/701 [00:00<00:00, 10783.18 examples/s]\n",
      "Map:   0%|          | 0/99 [00:00<?, ? examples/s]Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-fb103a6da4cc846c.arrow\n",
      "\n",
      "Map:   0%|          | 0/99 [00:00<?, ? examples/s]Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-fb103a6da4cc846c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-fb103a6da4cc846c.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 99/99 [00:00<00:00, 8184.09 examples/s]\n",
      "Map:   0%|          | 0/204 [00:00<?, ? examples/s]Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-530fcfa9d4af2b48.arrow\n",
      "\n",
      "Map:   0%|          | 0/204 [00:00<?, ? examples/s]Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-530fcfa9d4af2b48.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 05:11:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bens/.cache/huggingface/datasets/Davlan___sib200/arz_Arab/0.0.0/38977a667f6fc264d5c26ec57a01e16db040b358/cache-530fcfa9d4af2b48.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 204/204 [00:00<00:00, 10590.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Number of labels: 7\n",
      "Categories: ['entertainment', 'geography', 'health', 'politics', 'science/technology', 'sports', 'travel']\n",
      "Dataset splits: ['train', 'validation', 'test']\n",
      "Training examples: 701\n",
      "Validation examples: 99\n",
      "Test examples: 204\n",
      "\n",
      "Sample data point:\n",
      "Text: 'trkyA mtHAwTħ bAlbHAr mn jhAt tlAtħ: bHr Ăyjh mn Alγrb ، wAlbHr AlÂswd mn AlšmAl wAlbHr AlÂbyD Almtw...'\n",
      "Category: geography\n",
      "Label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "transliterator = CharMapper.builtin_mapper('ar2hsb')\n",
    "\n",
    "# Load the SIB-200 dataset\n",
    "if config.data_dir is None:\n",
    "    # Load from HuggingFace Hub\n",
    "    assert config.language is not None\n",
    "    logger.info(f\"Loading SIB-200 dataset for language: {config.language}\")\n",
    "    raw_datasets = load_dataset(SIB_200_HF_ID, config.language, cache_dir=config.cache_dir)\n",
    "    \n",
    "    # Create label mapping\n",
    "    categories = sorted(raw_datasets[\"train\"].unique(\"category\"))\n",
    "    category2id = {category: idx for idx, category in enumerate(categories)}\n",
    "    add_label_id = lambda example: {\"label\": category2id[example[\"category\"]]}\n",
    "    raw_datasets = raw_datasets.map(add_label_id)\n",
    "    transliterate_text = lambda example: {\"text\": transliterator(example[\"text\"])}\n",
    "    raw_datasets = raw_datasets.map(transliterate_text)\n",
    "else:\n",
    "    # Load from local directory\n",
    "    assert config.language is not None\n",
    "    lang_data_dir = os.path.join(os.path.abspath(config.data_dir), config.language)\n",
    "    raw_datasets = load_from_disk(lang_data_dir)\n",
    "    categories = sorted(raw_datasets[\"train\"].unique(\"category\"))\n",
    "    category2id = {category: idx for idx, category in enumerate(categories)}\n",
    "    add_label_id = lambda example: {\"label\": category2id[example[\"category\"]]}\n",
    "    raw_datasets = raw_datasets.map(add_label_id)\n",
    "    transliterate_text = lambda example: {\"text\": transliterator(example[\"text\"])}\n",
    "    raw_datasets = raw_datasets.map(transliterate_text)\n",
    "\n",
    "# Get labels and number of classes\n",
    "label_list = sorted(raw_datasets[\"train\"].unique(\"label\"))\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Categories: {categories}\")\n",
    "print(f\"Dataset splits: {list(raw_datasets.keys())}\")\n",
    "print(f\"Training examples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation examples: {len(raw_datasets['validation'])}\")\n",
    "print(f\"Test examples: {len(raw_datasets['test'])}\")\n",
    "\n",
    "# Show a sample from the dataset\n",
    "sample = raw_datasets[\"train\"][0]\n",
    "print(f\"\\nSample data point:\")\n",
    "print(f\"Text: '{sample['text'][:100]}...'\")\n",
    "print(f\"Category: {sample['category']}\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5d925",
   "metadata": {},
   "source": [
    "## 5. Model Setup\n",
    "\n",
    "Load and configure the PIXEL model for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef7d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:666] 2025-11-15 04:58:34,208 >> loading configuration file ../Team-PIXEL/pixel-m4/config.json\n",
      "[INFO|configuration_utils.py:720] 2025-11-15 04:58:34,210 >> Model config PIXELConfig {\n",
      "  \"_name_or_path\": \"../Team-PIXEL/pixel-m4\",\n",
      "  \"architectures\": [\n",
      "    \"PIXELForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"decoder_hidden_size\": 512,\n",
      "  \"decoder_intermediate_size\": 2048,\n",
      "  \"decoder_num_attention_heads\": 16,\n",
      "  \"decoder_num_hidden_layers\": 8,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"image_size\": [\n",
      "    16,\n",
      "    8464\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.25,\n",
      "  \"model_type\": \"pixel\",\n",
      "  \"norm_pix_loss\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0\"\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:720] 2025-11-15 04:58:34,210 >> Model config PIXELConfig {\n",
      "  \"_name_or_path\": \"../Team-PIXEL/pixel-m4\",\n",
      "  \"architectures\": [\n",
      "    \"PIXELForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"decoder_hidden_size\": 512,\n",
      "  \"decoder_intermediate_size\": 2048,\n",
      "  \"decoder_num_attention_heads\": 16,\n",
      "  \"decoder_num_hidden_layers\": 8,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"image_size\": [\n",
      "    16,\n",
      "    8464\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.25,\n",
      "  \"model_type\": \"pixel\",\n",
      "  \"norm_pix_loss\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2025 04:58:34 - INFO - __main__ - Using dropout with probability 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2531] 2025-11-15 04:58:34,213 >> loading weights file ../Team-PIXEL/pixel-m4/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:3180] 2025-11-15 04:58:35,677 >> Some weights of the model checkpoint at ../Team-PIXEL/pixel-m4 were not used when initializing PIXELForSequenceClassification: ['decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.mask_token', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.5.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3192] 2025-11-15 04:58:35,679 >> Some weights of PIXELForSequenceClassification were not initialized from the model checkpoint at ../Team-PIXEL/pixel-m4 and are newly initialized: ['pooler.linear.weight', 'pooler.ln.weight', 'classifier.weight', 'pooler.linear.bias', 'pooler.ln.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[WARNING|modeling_utils.py:3180] 2025-11-15 04:58:35,677 >> Some weights of the model checkpoint at ../Team-PIXEL/pixel-m4 were not used when initializing PIXELForSequenceClassification: ['decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.mask_token', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.5.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3192] 2025-11-15 04:58:35,679 >> Some weights of PIXELForSequenceClassification were not initialized from the model checkpoint at ../Team-PIXEL/pixel-m4 and are newly initialized: ['pooler.linear.weight', 'pooler.ln.weight', 'classifier.weight', 'pooler.linear.bias', 'pooler.ln.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model type: pixel\n",
      "Modality: Modality.IMAGE\n",
      "Number of parameters: 86,651,911\n",
      "Label mappings: {'entertainment': 0, 'geography': 1, 'health': 2, 'politics': 3, 'science/technology': 4, 'sports': 5, 'travel': 6}\n"
     ]
    }
   ],
   "source": [
    "# Setup model configuration\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": config.cache_dir,\n",
    "    \"revision\": config.model_revision,\n",
    "    \"use_auth_token\": config.use_auth_token if config.use_auth_token else None,\n",
    "}\n",
    "\n",
    "# Load model configuration\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    config.config_name if config.config_name else config.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    attention_probs_dropout_prob=config.dropout_prob,\n",
    "    hidden_dropout_prob=config.dropout_prob,\n",
    "    **config_kwargs,\n",
    ")\n",
    "\n",
    "logger.info(f\"Using dropout with probability {config.dropout_prob}\")\n",
    "\n",
    "# Load the model\n",
    "if model_config.model_type in [\"bert\", \"roberta\", \"xlm-roberta\"]:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name_or_path,\n",
    "        config=model_config,\n",
    "        **config_kwargs,\n",
    "    )\n",
    "    modality = Modality.TEXT\n",
    "elif model_config.model_type in [\"vit_mae\", \"pixel\"]:\n",
    "    pooling_mode = PoolingMode.from_string(config.pooling_mode)\n",
    "    model = PIXELForSequenceClassification.from_pretrained(\n",
    "        config.model_name_or_path,\n",
    "        config=model_config,\n",
    "        pooling_mode=pooling_mode,\n",
    "        add_layer_norm=config.pooler_add_layer_norm,\n",
    "        **config_kwargs,\n",
    "    )\n",
    "    modality = Modality.IMAGE\n",
    "else:\n",
    "    raise ValueError(f\"Model type {model_config.model_type} not supported.\")\n",
    "\n",
    "# Setup label mappings\n",
    "model.config.label2id = {l: i for i, l in enumerate(categories)}\n",
    "model.config.id2label = {i: l for i, l in enumerate(categories)}\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model type: {model_config.model_type}\")\n",
    "print(f\"Modality: {modality}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Label mappings: {model.config.label2id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixel-m4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
