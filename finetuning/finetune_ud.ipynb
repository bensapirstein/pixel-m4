{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792609de",
   "metadata": {},
   "source": [
    "# PIXEL-M4 Finetuning for Dependency Parsing\n",
    "\n",
    "This notebook demonstrates how to finetune a PIXEL-M4 model for Universal Dependencies (UD) dependency parsing task. It's based on the `run_ud_bigrams.py` script but configured for notebook use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68d926",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "First, let's set up all the configuration parameters that would normally be passed as command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d579a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: ../Team-PIXEL/pixel-m4\n",
      "Data directory: ../data/ud-treebanks-v2.10/UD_Arabic-PADT\n",
      "Output directory: ../debug/pixel-m4/udp/UD_Arabic-PADT/1e-5--0\n",
      "Learning rate: 1e-05\n",
      "Max steps: 15000\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Union\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configuration parameters (normally passed as command line arguments)\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model and data paths\n",
    "    model_name_or_path: str = \"../Team-PIXEL/pixel-m4\"\n",
    "    treebank: str = \"UD_Arabic-PADT\"\n",
    "    data_dir: str = f\"../data/ud-treebanks-v2.10/{treebank}\"\n",
    "    fallback_fonts_dir: str = \"../fallback_fonts\"\n",
    "    output_dir: str = f\"../debug/pixel-m4/udp/{treebank}/1e-5--0\"\n",
    "\n",
    "    # Training hyperparameters\n",
    "    seed: int = 0\n",
    "    learning_rate: float = 1e-5\n",
    "    max_seq_length: int = 256\n",
    "    max_steps: int = 15000\n",
    "    per_device_train_batch_size: int = 64\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    warmup_steps: int = 100\n",
    "    dropout_prob: float = 0.1\n",
    "    \n",
    "    # Training settings\n",
    "    do_train: bool = True\n",
    "    do_eval: bool = True\n",
    "    do_predict: bool = True\n",
    "    early_stopping: bool = True\n",
    "    early_stopping_patience: int = 5\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    logging_steps: int = 100\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    save_total_limit: int = 1\n",
    "    log_predictions: bool = True\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_las\"\n",
    "    \n",
    "    # Technical settings\n",
    "    rendering_backend: str = \"pangocairo\"\n",
    "    overwrite_output_dir: bool = True\n",
    "    overwrite_cache: bool = True\n",
    "    bf16: bool = True\n",
    "    dataloader_num_workers: int = 8\n",
    "    run_name: str = \"pixel-m4--notebook-debug\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config.model_name_or_path}\")\n",
    "print(f\"Data directory: {config.data_dir}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Max steps: {config.max_steps}\")\n",
    "print(f\"Batch size: {config.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc346d9",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import all the necessary libraries and components for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aee9e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bens/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "# PIXEL-specific imports\n",
    "from pixel import (\n",
    "    AutoConfig,\n",
    "    AutoModelForBiaffineParsing,\n",
    "    UD_HEAD_LABELS,\n",
    "    Modality,\n",
    "    PIXELTrainerForBiaffineParsing,\n",
    "    PIXELTrainingArguments,\n",
    "    Split,\n",
    "    PyGameTextRenderer,\n",
    "    UDDataset,\n",
    "    get_transforms,\n",
    "    resize_model_embeddings,\n",
    ")\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    PreTrainedTokenizerFast,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    PretrainedConfig,\n",
    ")\n",
    "\n",
    "# Rendering backend\n",
    "from pixel.data.rendering.pangocairo_renderer_bigrams_iso_char import PangoCairoTextRenderer as PangoCairoBigramsRenderer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set verbosity for all components\n",
    "datasets.utils.logging.set_verbosity(logging.INFO)\n",
    "transformers.utils.logging.set_verbosity(logging.INFO)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "print(\"All imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b12ca0",
   "metadata": {},
   "source": [
    "## Initialize Model and Configuration\n",
    "\n",
    "Load the model configuration and initialize the model for dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23796c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:666] 2025-11-16 15:39:29,561 >> loading configuration file ../Team-PIXEL/pixel-m4/config.json\n",
      "[INFO|configuration_utils.py:720] 2025-11-16 15:39:29,562 >> Model config PIXELConfig {\n",
      "  \"_name_or_path\": \"../Team-PIXEL/pixel-m4\",\n",
      "  \"architectures\": [\n",
      "    \"PIXELForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"decoder_hidden_size\": 512,\n",
      "  \"decoder_intermediate_size\": 2048,\n",
      "  \"decoder_num_attention_heads\": 16,\n",
      "  \"decoder_num_hidden_layers\": 8,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"_\",\n",
      "    \"1\": \"acl\",\n",
      "    \"2\": \"advcl\",\n",
      "    \"3\": \"advmod\",\n",
      "    \"4\": \"amod\",\n",
      "    \"5\": \"appos\",\n",
      "    \"6\": \"aux\",\n",
      "    \"7\": \"case\",\n",
      "    \"8\": \"cc\",\n",
      "    \"9\": \"ccomp\",\n",
      "    \"10\": \"clf\",\n",
      "    \"11\": \"compound\",\n",
      "    \"12\": \"conj\",\n",
      "    \"13\": \"cop\",\n",
      "    \"14\": \"csubj\",\n",
      "    \"15\": \"dep\",\n",
      "    \"16\": \"det\",\n",
      "    \"17\": \"discourse\",\n",
      "    \"18\": \"dislocated\",\n",
      "    \"19\": \"expl\",\n",
      "    \"20\": \"fixed\",\n",
      "    \"21\": \"flat\",\n",
      "    \"22\": \"goeswith\",\n",
      "    \"23\": \"iobj\",\n",
      "    \"24\": \"list\",\n",
      "    \"25\": \"mark\",\n",
      "    \"26\": \"nmod\",\n",
      "    \"27\": \"nsubj\",\n",
      "    \"28\": \"nummod\",\n",
      "    \"29\": \"obj\",\n",
      "    \"30\": \"obl\",\n",
      "    \"31\": \"orphan\",\n",
      "    \"32\": \"parataxis\",\n",
      "    \"33\": \"punct\",\n",
      "    \"34\": \"reparandum\",\n",
      "    \"35\": \"root\",\n",
      "    \"36\": \"vocative\",\n",
      "    \"37\": \"xcomp\"\n",
      "  },\n",
      "  \"image_size\": [\n",
      "    16,\n",
      "    8464\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"_\": 0,\n",
      "    \"acl\": 1,\n",
      "    \"advcl\": 2,\n",
      "    \"advmod\": 3,\n",
      "    \"amod\": 4,\n",
      "    \"appos\": 5,\n",
      "    \"aux\": 6,\n",
      "    \"case\": 7,\n",
      "    \"cc\": 8,\n",
      "    \"ccomp\": 9,\n",
      "    \"clf\": 10,\n",
      "    \"compound\": 11,\n",
      "    \"conj\": 12,\n",
      "    \"cop\": 13,\n",
      "    \"csubj\": 14,\n",
      "    \"dep\": 15,\n",
      "    \"det\": 16,\n",
      "    \"discourse\": 17,\n",
      "    \"dislocated\": 18,\n",
      "    \"expl\": 19,\n",
      "    \"fixed\": 20,\n",
      "    \"flat\": 21,\n",
      "    \"goeswith\": 22,\n",
      "    \"iobj\": 23,\n",
      "    \"list\": 24,\n",
      "    \"mark\": 25,\n",
      "    \"nmod\": 26,\n",
      "    \"nsubj\": 27,\n",
      "    \"nummod\": 28,\n",
      "    \"obj\": 29,\n",
      "    \"obl\": 30,\n",
      "    \"orphan\": 31,\n",
      "    \"parataxis\": 32,\n",
      "    \"punct\": 33,\n",
      "    \"reparandum\": 34,\n",
      "    \"root\": 35,\n",
      "    \"vocative\": 36,\n",
      "    \"xcomp\": 37\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.25,\n",
      "  \"model_type\": \"pixel\",\n",
      "  \"norm_pix_loss\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2531] 2025-11-16 15:39:29,564 >> loading weights file ../Team-PIXEL/pixel-m4/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dependency labels: 38\n",
      "Labels: ['_', 'acl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'case', 'cc', 'ccomp']...\n",
      "Model type: pixel\n",
      "Dropout probability: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:3180] 2025-11-16 15:39:30,884 >> Some weights of the model checkpoint at ../Team-PIXEL/pixel-m4 were not used when initializing PIXELForBiaffineParsing: ['decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.mask_token', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_after.bias']\n",
      "- This IS expected if you are initializing PIXELForBiaffineParsing from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PIXELForBiaffineParsing from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3192] 2025-11-16 15:39:30,886 >> Some weights of PIXELForBiaffineParsing were not initialized from the model checkpoint at ../Team-PIXEL/pixel-m4 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight', 'classifier.bias', 'biaffine_arcs.weight', 'classifier.weight', 'biaffine_rels.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully: PIXELForBiaffineParsing\n",
      "Model config pad_token_id: -100\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Prepare labels and label mapping\n",
    "labels = UD_HEAD_LABELS\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "print(f\"Number of dependency labels: {num_labels}\")\n",
    "print(f\"Labels: {labels[:10]}...\")  # Show first 10 labels\n",
    "\n",
    "# Initialize model configuration\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    config.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    attention_probs_dropout_prob=config.dropout_prob,\n",
    "    hidden_dropout_prob=config.dropout_prob,\n",
    ")\n",
    "\n",
    "# Set pad token ID for certain model types\n",
    "if model_config.model_type in [\"vit_mae\", \"pixel\", \"bert\"]:\n",
    "    model_config.pad_token_id = -100\n",
    "\n",
    "print(f\"Model type: {model_config.model_type}\")\n",
    "print(f\"Dropout probability: {config.dropout_prob}\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForBiaffineParsing.from_pretrained(\n",
    "    config.model_name_or_path,\n",
    "    config=model_config,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully: {type(model).__name__}\")\n",
    "print(f\"Model config pad_token_id: {model_config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed3d84",
   "metadata": {},
   "source": [
    "## Setup Text Processor\n",
    "\n",
    "Initialize the text processor (renderer for images or tokenizer for text) based on the model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe5af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using modality: Modality.IMAGE\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading font from /home/bens/pixel/configs/renderers/noto_renderer/GoNotoCurrent.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansManichaean-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTelugu-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAgrawImazighen-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCanadianAboriginal-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAzawagh-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSyriacWestern-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansAdlam-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKannada-Regular.ttf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTamilUI-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansChakma-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansJavanese-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSiddham-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMyanmar-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLinearA-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansArmenian-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansThaiUI-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGeorgian-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGurmukhiUI-Regular.ttf\n",
      "11/16/2025 15:39:30 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNushu-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCoptic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLycian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLydian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCypriot-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKhmer-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansEgyptianHieroglyphs-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMahajani-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSundanese-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoZnamennyMusicalNotation-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansElbasan-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOriya-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOsage-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGurmukhi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansThai-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCham-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMalayalam-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansShavian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSamaritan-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansDevanagari-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansEthiopic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansDeseret-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMarchen-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghTawellemmet-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansArabicUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSymbols2-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansHebrewDroid-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMeroitic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGujaratiUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBhaiksuki-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKayahLi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGunjalaGondi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoNaskhArabicUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSymbols-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinagh-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSinhala-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansThaana-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoFangsongKSSVertical-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansHanifiRohingya-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNabataean-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTirhuta-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTamil-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghGhat-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPahawhHmong-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGujarati-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMeeteiMayek-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLimbu-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldPermic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCJKkr-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPhoenician-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoNastaliqUrdu-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPhagsPa-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansThaiLooped-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansRunic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghRhissaIxa-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansInscriptionalPahlavi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansVai-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansAdlamUnjoined-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNewa-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCJKjp-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldHungarian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMiao-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansImperialAramaic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSogdian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTeluguUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansHebrew-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPalmyrene-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansIndicSiyaqNumbers-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansDuployan-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBuhid-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNagMundari-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLinearB-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKharoshthi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansElymaic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLaoLoopedUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansDuployan-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKhudawadi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAdrar-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansRejang-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMalayalamUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansWancho-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLao-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansDevanagariUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGlagolitic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansAnatolianHieroglyphs-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNKo-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/AppleColorEmoji.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldTurkic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSans-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoRashiHebrew-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTest-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMasaramGondi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMro-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCarian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCaucasianAlbanian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNandinagari-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMongolian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLisu-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAhaggar-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldPersian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCJKsc-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSunuwar-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAPT-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghSIL-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOlChiki-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMultani-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKannadaUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTakri-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansInscriptionalParthian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansUgaritic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTagbanwa-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSylotiNagri-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansChorasmian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansHatran-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPauCinHau-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKawi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldHungarianUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGrantha-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLepcha-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTamilSupplement-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMedefaidrin-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSyriacEastern-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoFangsongKSSRotated-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKaithi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSaurashtra-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTangsa-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCyproMinoan-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBuginese-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBatak-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCherokee-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoMusic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldSouthArabian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOgham-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOsmanya-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldSogdian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansThaiLoopedUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansModi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTaiLe-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMono-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCuneiform-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldItalic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansGothic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNewTaiLue-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMyanmarUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansPsalterPahlavi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTaiTham-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSoyombo-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBalinese-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCJKtc-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansWarangCiti-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLaoUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoTraditionalNushu-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansArabic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMayanNumerals-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansYi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoKufiArabic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBamum-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSignWriting-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSharada-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansVithkuqi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMath-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTagalog-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansNKoUnjoined-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansZanabazarSquare-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansCJKhk-Regular.otf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansLaoLooped-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansAvestan-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMendeKikakui-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansHanunoo-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansKhmerUI-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSoraSompeng-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansMandaic-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghAir-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTaiViet-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansOldNorthArabian-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansTifinaghHawad-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBassaVah-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansSyriac-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoSansBrahmi-Regular.ttf\n",
      "11/16/2025 15:39:31 - INFO - pixel.data.rendering.pangocairo_renderer_bigrams_iso_char - Loading fallback font ../fallback_fonts/NotoNaskhArabic-Regular.ttf\n",
      "Loaded text renderer: PangoCairoTextRenderer\n",
      "Updated processor max_seq_length to: 256\n",
      "11/16/2025 15:39:31 - INFO - pixel.utils.modeling - Truncating position embeddings to 256\n",
      "Resized model embeddings to match processor\n"
     ]
    }
   ],
   "source": [
    "# Determine modality based on model type\n",
    "modality = Modality.TEXT if model.config.model_type in [\"bert\", \"roberta\"] else Modality.IMAGE\n",
    "print(f\"Using modality: {modality}\")\n",
    "\n",
    "# Initialize processor based on modality\n",
    "if modality == Modality.TEXT:\n",
    "    processor = AutoTokenizer.from_pretrained(\n",
    "        config.model_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True if config.model_name_or_path == \"roberta-base\" else False,\n",
    "    )\n",
    "    print(f\"Loaded tokenizer: {type(processor).__name__}\")\n",
    "    \n",
    "elif modality == Modality.IMAGE:\n",
    "    # Use PangoCairo renderer for PIXEL models\n",
    "    # processor = PangoCairoBigramsRenderer.from_pretrained(\n",
    "    #     config.model_name_or_path,\n",
    "    #     fallback_fonts_dir=config.fallback_fonts_dir,\n",
    "    #     use_auth_token=True,\n",
    "    #     rgb=False,  # RGB rendering disabled for speed\n",
    "    # )\n",
    "    processor = PangoCairoBigramsRenderer(font_file=\"/home/bens/pixel/configs/renderers/noto_renderer/GoNotoCurrent.ttf\", font_size=6.5, fallback_fonts_dir=config.fallback_fonts_dir, rgb=False)\n",
    "    print(f\"Loaded text renderer: {type(processor).__name__}\")\n",
    "    # Update max sequence length if needed\n",
    "    if processor.max_seq_length != config.max_seq_length:\n",
    "        processor.max_seq_length = config.max_seq_length\n",
    "        print(f\"Updated processor max_seq_length to: {config.max_seq_length}\")\n",
    "    \n",
    "    # Resize model embeddings to match processor\n",
    "    resize_model_embeddings(model, processor.max_seq_length)\n",
    "    print(\"Resized model embeddings to match processor\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Modality {modality} not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f840a",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load the Universal Dependencies dataset for training, evaluation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28b5134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "11/16/2025 17:46:17 - INFO - pixel.data.datasets.ud_dataset - Creating features from dataset file at ../data/ud-treebanks-v2.10/UD_Arabic-PADT\n",
      "11/16/2025 17:46:17 - INFO - pixel.data.datasets.ud_dataset - Writing example 0 of 909\n",
      "11/16/2025 17:46:22 - INFO - pixel.data.datasets.ud_dataset - Saving features into cached file ../data/ud-treebanks-v2.10/UD_Arabic-PADT/cached_dev_PangoCairoTextRenderer_256\n",
      "11/16/2025 17:47:06 - INFO - pixel.data.datasets.ud_dataset - Creating features from dataset file at ../data/ud-treebanks-v2.10/UD_Arabic-PADT\n",
      "11/16/2025 17:47:06 - INFO - pixel.data.datasets.ud_dataset - Writing example 0 of 680\n",
      "11/16/2025 17:47:07 - WARNING - pixel.data.datasets.ud_dataset - Sequence of len 263 truncated: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ':', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '.', '', '', '', '', ':', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '.']\n",
      "11/16/2025 17:47:11 - INFO - pixel.data.datasets.ud_dataset - Saving features into cached file ../data/ud-treebanks-v2.10/UD_Arabic-PADT/cached_test_PangoCairoTextRenderer_256\n",
      "Evaluation dataset size: 909\n",
      "Test dataset size: 680\n",
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(split: Split):\n",
    "    \"\"\"Helper function to create datasets for different splits.\"\"\"\n",
    "    \n",
    "    if modality == Modality.IMAGE:\n",
    "        transforms = get_transforms(\n",
    "            do_resize=True,\n",
    "            size=(processor.pixels_per_patch, processor.pixels_per_patch * processor.max_seq_length),\n",
    "        )\n",
    "    else:\n",
    "        transforms = None\n",
    "\n",
    "    return UDDataset(\n",
    "        data_dir=config.data_dir,\n",
    "        processor=processor,\n",
    "        transforms=transforms,\n",
    "        modality=modality,\n",
    "        labels=UD_HEAD_LABELS,\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        overwrite_cache=config.overwrite_cache,\n",
    "        mode=split,\n",
    "        # allographic_augmentation=True,\n",
    "        pad_token=model_config.pad_token_id\n",
    "    )\n",
    "\n",
    "# Load datasets for different splits\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# train_dataset = get_dataset(Split.TRAIN) if config.do_train else None\n",
    "eval_dataset = get_dataset(Split.DEV) if config.do_eval else None  \n",
    "test_dataset = get_dataset(Split.TEST) if config.do_predict else None\n",
    "\n",
    "# if train_dataset:\n",
    "#     print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "if eval_dataset:\n",
    "    print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "if test_dataset:\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "print(\"Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a626ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def tensor_to_numpy(pixel_values):\n",
    "    if isinstance(pixel_values, torch.Tensor):\n",
    "        if pixel_values.dim() == 3:  # [C, H, W]\n",
    "            if pixel_values.shape[0] == 3:  # RGB\n",
    "                return pixel_values.permute(1, 2, 0).cpu().numpy()\n",
    "            else:  # Grayscale\n",
    "                return pixel_values[0].cpu().numpy()\n",
    "        else:\n",
    "            return pixel_values.cpu().numpy()\n",
    "    return np.array(pixel_values)\n",
    "\n",
    "def crop_to_text_width(img_array, num_patches, patch_width=16):\n",
    "    text_width = int(num_patches * patch_width)\n",
    "    if img_array.shape[1] > text_width:\n",
    "        if len(img_array.shape) == 2:\n",
    "            return img_array[:, :text_width]\n",
    "        else:\n",
    "            return img_array[:, :text_width, :]\n",
    "    return img_array\n",
    "\n",
    "def add_patch_boundaries(img_array, num_patches, patch_width=16):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    img_with_boundaries = img_array.copy()\n",
    "    text_width = int(num_patches * patch_width)\n",
    "    \n",
    "    # Add dashed vertical lines at patch boundaries\n",
    "    for patch_start in range(patch_width, text_width, patch_width):\n",
    "        if patch_start < img_array.shape[1]:\n",
    "            # Create dashed line pattern (every 3rd pixel) with alpha blending\n",
    "            for y in range(0, img_array.shape[0], 3):\n",
    "                if y < img_array.shape[0]:\n",
    "                    if len(img_array.shape) == 2:  # Grayscale\n",
    "                        # Alpha blend with existing pixel\n",
    "                        original = img_with_boundaries[y, patch_start]\n",
    "                        img_with_boundaries[y, patch_start] = 0.5 * 0.5 + original * 0.5\n",
    "                    else:  # RGB\n",
    "                        # Alpha blend with existing pixel\n",
    "                        original = img_with_boundaries[y, patch_start, :]\n",
    "                        line_color = np.array([1, 0.4, 0.4])\n",
    "                        img_with_boundaries[y, patch_start, :] = line_color * 0.5 + original * 0.5\n",
    "\n",
    "    return img_with_boundaries\n",
    "\n",
    "def example_to_image(example):\n",
    "    pixel_values = example['pixel_values']\n",
    "    num_patches = example['attention_mask'].sum()\n",
    "    \n",
    "    img_array = tensor_to_numpy(pixel_values)\n",
    "    img_array = crop_to_text_width(img_array, num_patches)\n",
    "    img_array = add_patch_boundaries(img_array, num_patches)\n",
    "    \n",
    "    # Convert to PIL Image for saving\n",
    "    if len(img_array.shape) == 2:  # Grayscale\n",
    "        img = Image.fromarray((img_array * 255).astype(np.uint8), mode='L')\n",
    "    else:  # RGB\n",
    "        img = Image.fromarray((img_array[:,:500,:] * 255).astype(np.uint8))\n",
    "    return img\n",
    "\n",
    "plt.imshow(example_to_image(eval_dataset[1]))\n",
    "plt.show()\n",
    "plt.imshow(example_to_image(test_dataset[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80f761ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 4096])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_7[5][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54ef32",
   "metadata": {},
   "source": [
    "## Setup Training Arguments and Trainer\n",
    "\n",
    "Configure the training arguments and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c776f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1194] 2025-11-01 18:47:30,622 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1537] 2025-11-01 18:47:30,726 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured:\n",
      "Output directory: ../debug/pixel-m4/udp/UD_Japanese-GSD/1e-5--0\n",
      "Max steps: 15000\n",
      "Learning rate: 1e-05\n",
      "Batch size: 64\n",
      "Early stopping: True\n"
     ]
    }
   ],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    \"\"\"Compute UAS and LAS metrics for dependency parsing.\"\"\"\n",
    "    arc_labels, rel_labels = p.label_ids\n",
    "    arc_preds, rel_preds = p.predictions\n",
    "\n",
    "    correct_arcs = np.equal(arc_preds, arc_labels)\n",
    "    correct_rels = np.equal(rel_preds, rel_labels)\n",
    "    correct_arcs_and_rels = correct_arcs * correct_rels\n",
    "\n",
    "    unlabeled_correct = correct_arcs.sum()\n",
    "    labeled_correct = correct_arcs_and_rels.sum()\n",
    "    total_words = correct_arcs.size\n",
    "\n",
    "    unlabeled_attachment_score = unlabeled_correct / total_words\n",
    "    labeled_attachment_score = labeled_correct / total_words\n",
    "\n",
    "    return {\n",
    "        \"uas\": unlabeled_attachment_score * 100,\n",
    "        \"las\": labeled_attachment_score * 100,\n",
    "    }\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = PIXELTrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    run_name=config.run_name,\n",
    "    do_train=config.do_train,\n",
    "    do_eval=config.do_eval,\n",
    "    do_predict=config.do_predict,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    max_steps=config.max_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    load_best_model_at_end=config.load_best_model_at_end,\n",
    "    metric_for_best_model=config.metric_for_best_model,\n",
    "    overwrite_output_dir=config.overwrite_output_dir,\n",
    "    seed=config.seed,\n",
    "    bf16=config.bf16,\n",
    "    half_precision_backend=\"cuda_amp\",\n",
    "    dataloader_num_workers=config.dataloader_num_workers,\n",
    "    report_to=\"none\",  # Disable wandb for notebook\n",
    "    log_predictions=config.log_predictions,\n",
    "    label_names=[\"arc_labels\", \"rel_labels\"],  # Specific to dependency parsing\n",
    "    early_stopping=config.early_stopping,\n",
    "    early_stopping_patience=config.early_stopping_patience,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Max steps: {training_args.max_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Early stopping: {training_args.early_stopping}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:564] 2025-11-01 18:47:31,170 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:621] 2025-11-01 18:47:31,171 >> Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Model parameters: 109,526,860\n",
      "Trainable parameters: 109,526,860\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "trainer = PIXELTrainerForBiaffineParsing(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if config.do_train else None,\n",
    "    eval_dataset=eval_dataset if config.do_eval else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)]\n",
    "    if config.early_stopping\n",
    "    else None,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ba7f8",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Now let's start the finetuning process! This will train the model on the dependency parsing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b591ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 15000 steps\n",
      "Evaluation every 500 steps\n",
      "Saving model every 500 steps\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer - ***** Running training *****\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Num examples = 7,050\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Num Epochs = 536\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Instantaneous batch size per device = 64\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Gradient Accumulation steps = 1\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Total optimization steps = 15,000\n",
      "11/01/2025 18:47:31 - INFO - pixel.trainer -   Number of trainable parameters = 109,526,860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bens/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 2; 23.59 GiB total capacity; 136.50 MiB already allocated; 8.12 MiB free; 144.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation every \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39meval_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model every \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39msave_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add training dataset size to metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pixel-m4/src/pixel/trainer.py:530\u001b[0m, in \u001b[0;36mPIXELTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m    528\u001b[0m         tr_loss_step, outputs_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m     tr_loss_step, outputs_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step):\n\u001b[1;32m    533\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/pixel-m4/src/pixel/trainer.py:177\u001b[0m, in \u001b[0;36mPIXELTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m--> 177\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/pixel-m4/src/pixel/trainer.py:714\u001b[0m, in \u001b[0;36mPIXELTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# debug_log_inputs(inputs)\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:170\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:175\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, device_ids):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/replicate.py:91\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     89\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     90\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m---> 91\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m     94\u001b[0m buffers_rg \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/replicate.py:71\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m \u001b[43mBroadcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tensor_copies[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/miniconda3/envs/pixel-m4/lib/python3.9/site-packages/torch/nn/parallel/comm.py:58\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     57\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 2; 23.59 GiB total capacity; 136.50 MiB already allocated; 8.12 MiB free; 144.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "if config.do_train:\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training for {config.max_steps} steps\")\n",
    "    print(f\"Evaluation every {config.eval_steps} steps\")\n",
    "    print(f\"Saving model every {config.save_steps} steps\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    \n",
    "    # Add training dataset size to metrics\n",
    "    metrics[\"train_samples\"] = len(train_dataset)\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Log and save training metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Final training loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "    print(f\"Training runtime: {metrics.get('train_runtime', 'N/A'):.2f} seconds\")\n",
    "    print(f\"Samples per second: {metrics.get('train_samples_per_second', 'N/A'):.2f}\")\n",
    "else:\n",
    "    print(\"Training skipped (do_train=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4894a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model on the development and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on development set\n",
    "if config.do_eval and eval_dataset:\n",
    "    print(\"*** Evaluating on development set ***\")\n",
    "    \n",
    "    outputs = trainer.predict(test_dataset=eval_dataset, metric_key_prefix=\"eval\")\n",
    "    metrics = outputs.metrics\n",
    "    \n",
    "    # Add evaluation dataset size to metrics\n",
    "    metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "    \n",
    "    # Log and save evaluation metrics\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    \n",
    "    print(f\"Evaluation UAS: {metrics.get('eval_uas', 'N/A'):.2f}%\")\n",
    "    print(f\"Evaluation LAS: {metrics.get('eval_las', 'N/A'):.2f}%\")\n",
    "    print(f\"Evaluation loss: {metrics.get('eval_loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Save predictions if requested\n",
    "    if config.log_predictions:\n",
    "        # Note: In a real scenario, you'd implement the log_predictions function\n",
    "        # from the original script. For simplicity, we're just noting this here.\n",
    "        print(\"Predictions would be saved to eval_predictions.csv\")\n",
    "else:\n",
    "    print(\"Evaluation skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on test set\n",
    "if config.do_predict and test_dataset:\n",
    "    print(\"*** Testing on test set ***\")\n",
    "    \n",
    "    outputs = trainer.predict(test_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "    metrics = outputs.metrics\n",
    "    \n",
    "    # Add test dataset size to metrics\n",
    "    metrics[\"test_samples\"] = len(test_dataset)\n",
    "    \n",
    "    # Log and save test metrics\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)\n",
    "    \n",
    "    print(f\"Test UAS: {metrics.get('test_uas', 'N/A'):.2f}%\")\n",
    "    print(f\"Test LAS: {metrics.get('test_las', 'N/A'):.2f}%\")\n",
    "    print(f\"Test loss: {metrics.get('test_loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Save predictions if requested\n",
    "    if config.log_predictions:\n",
    "        # Note: In a real scenario, you'd implement the log_predictions function\n",
    "        # from the original script. For simplicity, we're just noting this here.\n",
    "        print(\"Predictions would be saved to test_predictions.csv\")\n",
    "else:\n",
    "    print(\"Testing skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38da7a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training completed! Here's a summary of what we accomplished:\n",
    "\n",
    "1. **Configuration**: Set up all training parameters in a notebook-friendly way\n",
    "2. **Model Loading**: Loaded and configured the PIXEL-M4 model for dependency parsing\n",
    "3. **Data Processing**: Set up the text renderer/tokenizer and loaded UD datasets\n",
    "4. **Training**: Finetuned the model with early stopping and evaluation metrics\n",
    "5. **Evaluation**: Evaluated the model on both development and test sets\n",
    "\n",
    "The trained model is saved in the output directory and can be used for dependency parsing tasks.\n",
    "\n",
    "### Key Results:\n",
    "- **UAS (Unlabeled Attachment Score)**: Measures how well the model identifies syntactic heads\n",
    "- **LAS (Labeled Attachment Score)**: Measures how well the model identifies both heads and dependency relations\n",
    "\n",
    "### Next Steps:\n",
    "- Analyze the saved predictions in the CSV files\n",
    "- Experiment with different hyperparameters\n",
    "- Try different Universal Dependencies treebanks\n",
    "- Use the trained model for inference on new text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixel-m4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
